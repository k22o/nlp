# 大規模言語モデル(LLM)

## LLMとは

- 自然言語を用いて事前学習を行った大規模なニューラルネットワークのモデル
- 基本的には、Transformerを用いて作成される
- 以下のような例がある
  - GPT: Transformerのデコーダのみを用いて作成
  - BERT/RoBERTa: Transformerのエンコーダのみを用いて作成
  - T5: Transformerのエンコーダとデコーダを用いて作成

## LLMの活用方法

LLMは、事前学習済み (pre-trained) なモデルである。
こうしたpre-trainedなモデルを利用したタスクを**下流タスク**と呼ぶ。
こうした下流タスクの解き方は、いくつかに大別される。

(a) プロンプトを通した利用

- プロンプトを通じて下流タスクを解く (chat-gptなど)。
- すなわち、何らかの文章を入力として与え、それに対して答えてもらう形式。

(b) ファインチューニングによる新たなモデル構築

- 下流タスクのデータセットでファインチューニングして、新たなモデルを構築する。すなわち、**転移学習 (transfer learning)** をしている。
- 既存のままでは性能が出ない場合、独自の能力を組み込みたい場合などに利用される

LLMをそのままプロンプトとして利用する以上に性能を上げる方法が複数ある。次項以降で記載する。

## ファインチューニングなしで行うプロンプトの工夫

ファインチューニングなど、モデルの学習をせずに利用する場合に、性能を向上させるテクニック。一番簡単。

- **例示による学習**
  - プロンプトを作成する際に、例を示すことで正答率が向上することが知られている
  - (e.g.) 例に倣って日本語になおしてください。 例) Apple -> リンゴ 問) Oragen -> ?
  - 以下のように分類される
    - zero-shot learning: 例示なし
    - one-shot learning: 1件例示
    - few-shot learning: 複数件例示
       - 文脈からデータを学ばせており、「**文脈内学習**」と呼ばれる
- **Chain-of-Thought推論**
  - 多段階の推論が要求される課題 (マルチステップ推論) はLLMが苦手とするところ
　- その推論過程を例示することで、性能が向上することがわかっており、これを**Chain-of-Thought推論**と呼ぶ


## ファインチューニング

前述の通り、下流タスクのデータを利用し、モデルを再構築する。
後述する、RLHFと比較する際には、Supervised ファインチューニングとも呼ばれる。
(RLHFも、広義ファインチューニングのため)。
ファインチューニングで用いるデータセットは、もとのLLMに比べて少量のデータセットで済む。

### ファインチューニングの手順

(1) 学習に必要なデータを用意し、適した形に変換する

- 下流タスクで必要なデータを使う
- 一般的な機械学習に同じく、訓練データとテストデータに分割する
- 分割したデータの特徴などを、必要に応じて分析する
- (2) で選択するモデルに適したデータ形式に変換する
  - LLMに入れる上では、”tokenize" が必須となる

(2) pre-trainingされたLLMモデルを選択する

- GPT, BERTなど、必要となるLLMモデルを選択し、それらを実行できる環境を用意する

(3) 学習する

- (1)のデータをもとに、モデルを更新する
  - どのように学習を進めるかは手法による (後述)
- LLMのフレームワークでは、学習自体も組み込まれており、Tensorflowなどの利用は不要になっている

(4) 評価する

- 学習結果を評価、分析する
  - テキスト分類や固有表現抽出では、評価が0/1でわかりやすい
  - 要約などの事例では、評価をどのようにするかも重要になってくる

### ファインチューニングの更新則

パラメータの更新方法には、すべてのパラメータを更新する場合と一部を更新する場合がある。

- Full Fine-Tuning
  - 全パラメータについて更新
  - 大規模なリソースが必要
- Parameter Efficient Fine-Tuning (詳細は後述)
  - 追加したパラメータや一部のパラメータのみを更新する
  - 大規模なリソースは不要
  - 基本はこっちが優勢

### Parameter Efficient Fine-Tuningの分類

更新ルールに基づいた、ファインチューニングの分類。以下のような手法に大別される。現状は、Reparametarizationタイプが最優秀。

- Adaptorタイプ (e.g. Adaptor)
  - transformer内に、MLPを追加してそれだけ学習する
- Soft Promptタイプ (e.g. Prompt Tuning)
  - 入力にタスクごとのベクトル (Soft Prompt) を付加して学習を実施
- Selectiveタイプ (e.g. BitFit)
  - pre-trainedなモデルの一部のパラメータのみを更新 
- Reparametarizationタイプ (e.g. LoRA)
  - 行列分解に基づき、再パラメータ化された重みについて学習を実施


### 指示チューニング (instruction tuning)

- ファインチューニングの一種
- 指示を含んだプロンプトと理想的な回答のペアを使って学習する
- 以下のような欠点もある
  - データセットの用意が大変
  - モデルの回答をフィードバックさせることができない
  - 何かを言わないようにする、という正解データの作成が難しい

### RLHF (Reinforcement Learning from Human Feedback)

- ファインチューニングの一種 (別物とする場合もある)
- 出力を人間が評価して、それを報酬としてフィードバックする形で、強化学習的に学習を進める
- 以下のような欠点がある
  - 指示チューニングに比べて学習が難しい、という欠点がある
  - フィードバックの種類と効率のトレードオフ
  - RLHFの結果として、高学歴・高所得の意見を反映するようになってしまうことがある

## アライメント

LLMの出力が、人間や社会にとって有益となるように調整することを**alignment**と呼ぶ。
これを達成できるように、ファインチューニングなどを行う必要がある。「有益」には3つの基準 (3H) がある。

- helpful (役立つこと)
- honest (正直であること)
  - モデルが虚偽の生成をする (hallucination) ことがないようにする
- harmless (無害であること)
  - 犯罪の原因になりかねない助言などをしないように、など

## Augmented Language Model

LLMは、信頼性や知識の更新の観点で問題がある。そこで、外部のツールも併用することで、性能を上げることができる。

- Retrivial Augmented LM: 外部データ (検索など) + LLM
- Tool Augumented LM: 外部ツール + LLM

## 参考

「大規模言語モデル入門」 技術評論社