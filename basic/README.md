# Natual Language Processing / 自然言語処理

## NLPの代表的なタスク

- キスト分類/文書分類
  - テキストをカテゴリーに分類する
  - 例: スパム検出、感情分析、トピック分類
- 意味解析/意味理解
  - 文章の意味や意図を理解し、解釈する
- 固有表現抽出/固有表現認識
  - テキスト内から人名、場所、日付、組織などの固有表現 (named entity) を抽出する
- 機械翻訳
  - 一つの言語から別の言語への翻訳を行う
- 要約
  - 長い文章を短くまとめる
- 文埋め込み/文表現
  - 文の意味をベクトル化する
  - 生成されたベクトルは、文ペアの意味的類似度計算や転移学習の特徴量として利用される
- 発話理解/音声認識
  - 音声データをテキストに変換する
- 質問応答
  - ユーザーの質問に対して適切な回答を生成する
- 言語モデリング
  - 文章や文の予測、生成、評価を行うモデルの構築や活用を目指す
- 文法解析/構文解析
  - 文章の構造を解析して、文法的な関係や構文木を作成する
- 文書生成
  - 指定された条件やコンテキストに基づいて文章を生成する

## NLPの技術基盤

(1) 自然言語の構造に基づく分析・応用  
チョムスキーの文法理論など、構文解析をする上での基礎的な論理体系に基づく。

(2) 機械学習や統計的手法  
HMM (Hidden Markov Model) や、Nグラムモデルなどの利用

(3) 深層学習  
Seq2Seq、RNN、Transformer、LLMなど

## NLPのタスクの詳細

### 固有表現抽出 (Nemed Entity Extraction) / 固有表現認識 (Nemed Entity Recognition)

固有表現認識タスクの代表例として以下のものが存在する

- Flat NER
  - テキスト上で固有表現ラベルが重複せず、連続した固有表現しかないとする設定でのタスク。最も基本的。
  - 例: 「Aさん」は、「B市」出身だ
- Nested NER
  - テキスト上で固有表現ラベルが重複する場合があるが、連続した固有表現しかないとする設定のタスク。
  - 例: 「A県庁は」の場合は、「A県」という地名と「A県庁」という施設名が重複
- Discontinuous NER
  - テキスト上で固有表現ラベルが不連続する場合を想定したタスク
  - 例: 「5/26および27」といった場合は「5/27」という時間表現が不連続になっている

また、主要なアプローチとして、以下のようなものがある。

- 系列ラベリングアプローチ
  - トークン列の各要素に、固有表現のラベリングをする
  - 例: 5トークンの文字列なら、「人名 x 施設名 x x」
- スパンベースアプローチ
  - トークン列の任意の範囲についてラベリングする
  - Nested NERと相性がよい
  - 例: 「A県/庁/は」の場合... (1,1): 地名, (1,2): 施設名, (1,3): x
- 生成型アプローチ
　- トークン列の位置とラベルを含むデータを生成する
　- Nested NERやDiscontinuous NERに対応できる
  - 例: 「A県/庁/は」の場合... (1,1): 地名, (1,2): 施設名


### 文埋め込み (sentence embedding) / 文表現 (sentence representation)

以下のような実現方法がある

- 単語埋め込みの足し合わせ
  - word2vecを利用して算出した単語の埋め込みを足し合わせる/平均する
  - 単純ながら高性能を出すことが多い
  - 単純が故、意味が異なる文対しても高い類似どを出す可能性がある
- LLMの出力結果を利用する
  - LLMの `[CLS]` tokenのベクトル値を利用したり、文を構成するベクトルの平均やmax-poolingを利用する
  - 前述の手法より精度が落ちる
- SimCSE (Simple Contrastive learning of Sentense Embedding)
  - **対照学習 (Contrastive learning)**と呼ばれる手法を使う。
    - 類似した事例のペア (正例ペア) と 異なる事例のペア (負例ペア) を比較して学習を行う
    - 正例ならベクトル類似度が大きく、負例なら小さくなるように学習する
  - 教師なしSimCSEと教師ありSimCSEがある