# 埋め込み/分散表現 (Embedding), 特徴量

embeddingといいつつ、単語の特徴量評価とか、その辺も書く。

## 単語埋め込み (Word Embedding)

単語を数値ベクトルで表現する技術を、単語埋め込み（word embedding）と呼ぶ。単語埋め込みは、テキストデータ内の単語を密なベクトル空間にマッピングすることで、単語間の意味的な関係や文脈の情報を捉えることを可能にする。

単語埋め込みの主な特徴は以下の通り。

- 似た意味を持つ単語は、ベクトル空間上で近い位置にマッピングされる
- その単語が現れる文脈情報を捉えることができる (同じ単語でも意味の違いを判別できる)
- 高次元の単語の空間を低次元の密なベクトル空間に写像するため、次元削減できる
  - one-hot vectorだと、次元がかなり大きい

代表的な単語埋め込み手法は以下の通り。最近であれば、

- **Word2Vec**: CBOW (Continuous Bag of Words) とSkip-gramの2つのモデルからなる、ニューラルネットワークベースの単語埋め込み手法
- **GloVe (Global Vectors for Word Representation)**: カウントベースの手法であり、大規模なコーパスから得られた単語の共起行列を用いて単語間の関連性を学習
- **fastText**: Subword information（単語の内部構造）を考慮することで、未知語への対応を強化した手法
- **BERT**: BERTに文書を入力すれば、trainingなしでembeddingした結果を出力してくれる。もうこれでいいんでは。

## 文埋め込み/文表現 (Sentence Embedding)

文の意味をベクトル化する。生成されたベクトルは、文ペアの意味的類似度計算や転移学習の特徴量として利用される。

以下のような実現方法がある。

- 単語埋め込みの足し合わせ
  - word2vecを利用して算出した単語の埋め込みを足し合わせる/平均する
  - 単純ながら高性能を出すことが多い
  - 単純が故、意味が異なる文対しても高い類似どを出す可能性がある
- LLMの出力結果を利用する
  - LLMの `[CLS]` tokenのベクトル値を利用したり、文を構成するベクトルの平均やmax-poolingを利用する
  - 前述の手法より精度が落ちる
- SimCSE (Simple Contrastive learning of Sentense Embedding)
  - **対照学習 (Contrastive learning)**と呼ばれる手法を使う。
    - 類似した事例のペア (正例ペア) と 異なる事例のペア (負例ペア) を比較して学習を行う
    - 正例ならベクトル類似度が大きく、負例なら小さくなるように学習する
  - 教師なしSimCSEと教師ありSimCSEがある

## Bag of Words

embeddingではないが、文章のベクトル化手法として、Bag of Wordsを取りあげる。

Bag of Wordsとは、文章中の単語の出現数を用いて、文章をベクトル化する方法。単語の出現数のみに着目するため、単語や文章の並びは考慮しない。頻出する助詞や助動詞は単語だけでの意味を持たないため、ストップワードとして除去することで、数値が大きくても重要と判定されないようにする。

## TF-IDF（Term Frequency-Inverse Document Frequency）

単語の重要度を評価するための手法。これもembeddingではない。
TFとIDFを組み合わせて、特定の文書内で頻繁に出現する単語（TFが高い）かつ他の文書ではあまり使われていない単語（IDFが高い）が、その文書内で重要な情報を持つようにした指標。

- TF（Term Frequency：単語の出現頻度）
  - ある文書内での特定の単語の出現回数を表す
  - 文書内でよく出現する単語はその文書の内容に関連する可能性が高いと考得られる
- IDF（Inverse Document Frequency：逆文書頻度）
  - 単語の希少性を示す指標
  - ある単語が複数の文書に出現する頻度の逆数を取る
  - IDFは、特定の単語が一般的でなく、重要な情報を含んでいる可能性が高いことを示す